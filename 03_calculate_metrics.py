"""
03: Calculate performance metrics from simulation outputs.

This script extracts performance metrics from each simulation output file
and saves them for sensitivity analysis.

Also calculates baseline metrics from the pre-simulated baseline run.

Usage:
    python 03_calculate_metrics.py
"""

import sys
from pathlib import Path

# Add methods to path
sys.path.insert(0, str(Path(__file__).parent))

from config import METRICS_TO_CALCULATE, SIMULATIONS_DIR, METRICS_DIR, PRESIM_DIR
from methods.metrics import (
    calculate_all_metrics,
    calculate_sample_metrics,
    save_metrics,
)
from methods.simulation import load_simulation_results

# Baseline output file (generated by 00_generate_presimulated_releases.py)
BASELINE_OUTPUT_FILE = PRESIM_DIR / "full_model_baseline_output.hdf5"


def calculate_baseline_metrics():
    """
    Calculate and save baseline metrics from the pre-simulated baseline run.

    Returns
    -------
    dict or None
        Dictionary of baseline metrics, or None if baseline not available
    """
    import pandas as pd

    if not BASELINE_OUTPUT_FILE.exists():
        print("\nBaseline output file not found:")
        print(f"  {BASELINE_OUTPUT_FILE}")
        print("  Run 00_generate_presimulated_releases.py first.")
        return None

    print("\nCalculating baseline metrics...")
    print(f"  Source: {BASELINE_OUTPUT_FILE}")

    try:
        baseline_metrics = calculate_sample_metrics(
            sample_id=-1,  # Use -1 to indicate baseline
            output_file=str(BASELINE_OUTPUT_FILE),
            metrics=METRICS_TO_CALCULATE
        )

        # Remove sample_id for cleaner output
        baseline_metrics_clean = {k: v for k, v in baseline_metrics.items() if k != 'sample_id'}

        # Print baseline metrics
        print("\n  Baseline Metrics:")
        for name, value in baseline_metrics_clean.items():
            print(f"    {name}: {value:.4f}")

        # Save baseline metrics to CSV
        baseline_df = pd.DataFrame([baseline_metrics_clean])
        baseline_file = METRICS_DIR / "baseline_metrics.csv"
        baseline_df.to_csv(baseline_file, index=False)
        print(f"\n  Saved baseline metrics to: {baseline_file}")

        return baseline_metrics_clean

    except Exception as e:
        print(f"\nError calculating baseline metrics: {e}")
        return None


def main():
    """Calculate metrics for all completed simulations."""

    print("=" * 70)
    print("PERFORMANCE METRICS CALCULATION")
    print("=" * 70)

    # List available metrics
    print("\nConfigured metrics:")
    for m in METRICS_TO_CALCULATE:
        print(f"  - {m}")

    # Load simulation results if available
    try:
        simulation_results = load_simulation_results()
        n_total = len(simulation_results)
        n_success = (simulation_results["status"] == "success").sum()
        print(f"\nLoaded simulation results:")
        print(f"  Total samples: {n_total}")
        print(f"  Successful: {n_success}")
    except FileNotFoundError:
        print("\nNo simulation results file found, scanning directory...")
        simulation_results = None

    # Calculate metrics
    print("\nCalculating metrics...")
    metrics_df = calculate_all_metrics(simulation_results, metrics=METRICS_TO_CALCULATE)

    # Save metrics
    save_metrics(metrics_df)

    # Print summary statistics
    print("\n" + "=" * 70)
    print("METRICS SUMMARY")
    print("=" * 70)

    for col in metrics_df.columns:
        if col != "sample_id":
            data = metrics_df[col].dropna()
            print(f"\n{col}:")
            print(f"  Count: {len(data)}")
            print(f"  Mean:  {data.mean():.4f}")
            print(f"  Std:   {data.std():.4f}")
            print(f"  Min:   {data.min():.4f}")
            print(f"  Max:   {data.max():.4f}")

    # Calculate baseline metrics
    print("\n" + "-" * 70)
    print("BASELINE METRICS")
    print("-" * 70)
    calculate_baseline_metrics()

    print("\n" + "=" * 70)
    print("METRICS CALCULATION COMPLETE")
    print("=" * 70)
    print(f"\nMetrics saved to: {METRICS_DIR}")
    print(f"Next step: Run sensitivity analysis with 04_analyze_sensitivity.py")


if __name__ == "__main__":
    main()
