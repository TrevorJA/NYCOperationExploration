"""
03: Calculate performance metrics from simulation outputs.

This script extracts performance metrics from each simulation output file
and saves them for sensitivity analysis.

Also calculates baseline metrics from the pre-simulated baseline run.

Usage:
    python 03_calculate_metrics.py
    mpirun -np 4 python 03_calculate_metrics.py
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd
from mpi4py import MPI

# Add methods to path
sys.path.insert(0, str(Path(__file__).parent))

from config import METRICS_TO_CALCULATE, SIMULATIONS_DIR, METRICS_DIR, PRESIM_DIR, START_DATE, END_DATE
from methods.metrics import (
    calculate_sample_metrics,
)
from methods.simulation import load_simulation_results

# MPI setup
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Baseline output file (generated by 00_generate_presimulated_releases.py)
BASELINE_OUTPUT_FILE = PRESIM_DIR / "full_model_baseline_output.hdf5"


def calculate_baseline_metrics():
    """
    Calculate and save baseline metrics from the pre-simulated baseline run.

    Returns
    -------
    dict or None
        Dictionary of baseline metrics, or None if baseline not available
    """
    if not BASELINE_OUTPUT_FILE.exists():
        print("\nBaseline output file not found:")
        print(f"  {BASELINE_OUTPUT_FILE}")
        print("  Run 00_generate_presimulated_releases.py first.")
        return None

    print("\nCalculating baseline metrics...")
    print(f"  Source: {BASELINE_OUTPUT_FILE}")
    print(f"  Trimming data to config date range: {START_DATE} to {END_DATE}")

    try:
        baseline_metrics = calculate_sample_metrics(
            sample_id=-1,
            output_file=str(BASELINE_OUTPUT_FILE),
            metrics=METRICS_TO_CALCULATE,
            start_date=START_DATE,
            end_date=END_DATE
        )

        baseline_metrics_clean = {k: v for k, v in baseline_metrics.items() if k != 'sample_id'}

        print("\n  Baseline Metrics:")
        for name, value in baseline_metrics_clean.items():
            print(f"    {name}: {value:.4f}")

        baseline_df = pd.DataFrame([baseline_metrics_clean])
        baseline_file = METRICS_DIR / "baseline_metrics.csv"
        baseline_df.to_csv(baseline_file, index=False)
        print(f"\n  Saved baseline metrics to: {baseline_file}")

        return baseline_metrics_clean

    except Exception as e:
        print(f"\nError calculating baseline metrics: {e}")
        return None


def main():
    """Calculate metrics for all completed simulations using MPI."""

    # Rank 0 loads simulation results and distributes work
    if rank == 0:
        print("=" * 70)
        print(f"PERFORMANCE METRICS CALCULATION ({size} ranks)")
        print("=" * 70)

        print("\nConfigured metrics:")
        for m in METRICS_TO_CALCULATE:
            print(f"  - {m}")

        # Load simulation results
        try:
            simulation_results = load_simulation_results()
            successful = simulation_results[simulation_results["status"] == "success"]
            n_total = len(successful)
            print(f"\nLoaded {n_total} successful simulations")
        except FileNotFoundError:
            print("\nNo simulation results file found, scanning directory...")
            output_files = sorted(SIMULATIONS_DIR.glob("sample_*.hdf5"))
            successful = pd.DataFrame([
                {"sample_id": int(f.stem.split("_")[1]), "output_file": str(f)}
                for f in output_files
            ])
            n_total = len(successful)

        # Convert to list of (sample_id, output_file) tuples
        work_items = list(zip(successful["sample_id"], successful["output_file"]))
    else:
        work_items = None

    # Broadcast work items to all ranks
    work_items = comm.bcast(work_items, root=0)

    # Distribute work across ranks
    my_items = work_items[rank::size]

    if rank == 0:
        print(f"\nDistributing {len(work_items)} samples across {size} ranks")

    # Each rank processes its samples
    local_metrics = []
    for i, (sample_id, output_file) in enumerate(my_items):
        if rank == 0 and (i + 1) % 10 == 0:
            print(f"  Rank 0: processed {i + 1}/{len(my_items)} samples")

        metrics_dict = calculate_sample_metrics(
            sample_id, output_file, METRICS_TO_CALCULATE
        )
        local_metrics.append(metrics_dict)

    # Gather all results to rank 0
    all_metrics = comm.gather(local_metrics, root=0)

    # Rank 0 combines and saves
    if rank == 0:
        # Flatten list of lists
        combined = [m for rank_metrics in all_metrics for m in rank_metrics]
        metrics_df = pd.DataFrame(combined)

        # Sort by sample_id to maintain consistent ordering
        metrics_df = metrics_df.sort_values("sample_id").reset_index(drop=True)

        # Save metrics
        output_path = METRICS_DIR / "metrics.csv"
        metrics_df.to_csv(output_path, index=False)
        print(f"\nSaved metrics to {output_path}")

        # Print summary
        print("\n" + "=" * 70)
        print("METRICS SUMMARY")
        print("=" * 70)

        for col in metrics_df.columns:
            if col != "sample_id":
                data = metrics_df[col].dropna()
                print(f"\n{col}:")
                print(f"  Count: {len(data)}")
                print(f"  Mean:  {data.mean():.4f}")
                print(f"  Std:   {data.std():.4f}")
                print(f"  Min:   {data.min():.4f}")
                print(f"  Max:   {data.max():.4f}")

        # Calculate baseline metrics (only rank 0)
        print("\n" + "-" * 70)
        print("BASELINE METRICS")
        print("-" * 70)
        calculate_baseline_metrics()

        print("\n" + "=" * 70)
        print("METRICS CALCULATION COMPLETE")
        print("=" * 70)
        print(f"\nNext step: Run sensitivity analysis with 04_analyze_sensitivity.py")


if __name__ == "__main__":
    main()
